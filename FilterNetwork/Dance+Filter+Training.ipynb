{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "from os.path import isfile\n",
    "from glob import glob\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    'D:/School/hbo/sem4/OI/intelligentbeehive/examples/WDD_paper/GroundTruthData/GTRuns/wddGroundTruth/',\n",
    "    'D:/School/hbo/sem4/OI/intelligentbeehive/examples/WDD_paper/GroundTruthData/GTRuns/20160814_1001_1/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = list(itertools.chain.from_iterable(map(lambda path: glob(path + \"*/*/\"), image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.zeros((len(x1),)).astype(int)\n",
    "for i, x in enumerate(x1):\n",
    "    x2[i] = len(glob(x + '/image_*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/20170620_1606_FE.csv' does not exist: b'/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/20170620_1606_FE.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c02e0c6ea95f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcsv_path_16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/20170620_1606_FE.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgt_data_16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path_16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'gt_intern'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'gt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgt_data_16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgt_data_16\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mgt_data_16\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgt_data_16\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gt_intern'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgt_data_16\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\beehive\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\beehive\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\beehive\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\beehive\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\beehive\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/20170620_1606_FE.csv' does not exist: b'/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/20170620_1606_FE.csv'"
     ]
    }
   ],
   "source": [
    "csv_path_16 = '/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/20170620_1606_FE.csv'\n",
    "gt_data_16 = pd.read_csv(csv_path_16, header=None, names=['key', 'gt_intern', 'gt'], index_col=0)\n",
    "gt_data_16 = gt_data_16[~gt_data_16.index.duplicated(keep='first')]\n",
    "gt_data_16.drop('gt_intern', axis=1, inplace=True)\n",
    "gt_data_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_14 = '/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/GT_20160816.csv'\n",
    "gt_data_14 = pd.read_csv(csv_path_14, header=None, names=['key', 'gt'], index_col=0)\n",
    "gt_data_14 = gt_data_14[~gt_data_14.index.duplicated(keep='first')]\n",
    "gt_data_14.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = pd.concat((gt_data_14, gt_data_16), axis=0)\n",
    "gt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = []\n",
    "X2 = []\n",
    "Y = []\n",
    "for (i, path), nimages in zip(enumerate(x1), x2):\n",
    "    try:\n",
    "        label = int(gt_data.loc['/'.join(path.split('/')[-3:-1])]['gt'])\n",
    "        X1.append(path)\n",
    "        X2.append(nimages)\n",
    "        Y.append(label)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "x1 = np.array(X1)\n",
    "x2 = np.array(X2)\n",
    "y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_data_path = '/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/filter_data_rec_mixed.h5'\n",
    "\n",
    "if not isfile(filter_data_path):\n",
    "    with h5py.File(filter_data_path, 'w') as f:\n",
    "        g_X = f.create_group('X')\n",
    "        g_y = f.create_dataset('y', (len(y), ), np.int64)\n",
    "        f.create_dataset('n_samples', data=[len(y)])\n",
    "\n",
    "        for idx in trange(len(y)):\n",
    "            n_samples = x2[idx]\n",
    "\n",
    "            samples = []\n",
    "            for i in range(n_samples):\n",
    "                image_path = x1[idx] + \"image_\" + str(i).zfill(3) + \".png\"\n",
    "                samples.append(rgb2gray(io.imread(image_path)))\n",
    "\n",
    "            samples = np.swapaxes(np.swapaxes(np.array(samples), 0, 2), 0,1)[np.newaxis]\n",
    "            g_X.create_dataset('{}'.format(idx), data=samples.astype(np.float32))\n",
    "            g_y[idx] = y[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filter_data_path, 'r') as f:\n",
    "    n_samples = f['n_samples'][0]\n",
    "    g_X = f['X']\n",
    "    seq_lens = []\n",
    "    \n",
    "    for idx in trange(n_samples):\n",
    "        seq_lens.append(g_X['{}'.format(idx)].shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(seq_lens)\n",
    "print(np.max(seq_lens))\n",
    "print(np.min(seq_lens))\n",
    "print(np.median(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = list(range(len(seq_lens)))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:int(.8 * len(indices))]\n",
    "val_indices = indices[int(.8 * len(indices)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_gen(data, indices, batch_size, seq_len=128, augment=False, random_border_crop=4, cutoff=10):\n",
    "    assert(random_border_crop % 2 == 0)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    width = X['0'].shape[1]\n",
    "    heigth = X['0'].shape[2]\n",
    "    \n",
    "    while True:\n",
    "        batch_indices = np.random.choice(indices, replace=False, size=batch_size)\n",
    "        batch_samples = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            sample = X['{}'.format(idx)][:]\n",
    "            sample = sample[:, :, :, cutoff:-cutoff]\n",
    "            assert(sample.shape[-1] > 10)\n",
    "            if sample.shape[-1] > seq_len:\n",
    "                start_idx = np.random.randint(0, sample.shape[-1] - seq_len)\n",
    "                sample = sample[:, :, :, start_idx:start_idx+seq_len]\n",
    "            batch_samples.append(sample)\n",
    "            batch_labels.append(y[idx])\n",
    "            \n",
    "        X_b = np.zeros((batch_size, 1, width, heigth, seq_len), dtype=np.float32)\n",
    "        for idx, sample in enumerate(batch_samples):\n",
    "            X_b[idx, :, :, :, :sample.shape[-1]] = sample\n",
    "            \n",
    "        if augment:\n",
    "            crops = np.random.randint(0, random_border_crop, size=(X_b.shape[0], 2))\n",
    "            crops = [(slice(None, None, None),\n",
    "                      slice(crops[i, 0], -random_border_crop+crops[i, 0], None),\n",
    "                      slice(crops[i, 1], -random_border_crop+crops[i, 1], None),\n",
    "                      slice(None, None, None))\n",
    "                      for i in range(X_b.shape[0])]\n",
    "            flips = [(slice(None, None, None),\n",
    "                      slice(None, None, random.choice([-1, None])),\n",
    "                      slice(None, None, random.choice([-1, None])),\n",
    "                      slice(None, None, None))\n",
    "                      for i in range(X_b.shape[0])]\n",
    "            X_b = np.array([(seq[crop])[flip] for seq, crop, flip in zip(X_b, crops, flips)])\n",
    "        else:\n",
    "            crop = random_border_crop // 2\n",
    "            X_b = X_b[:, :, crop:-crop, crop:-crop, :]\n",
    "            \n",
    "        yield X_b, np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv3D, AveragePooling3D\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdamWithWeightnorm(Adam):\n",
    "    def get_updates(self, params, constraints, loss):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n",
    "\n",
    "        shapes = [K.get_variable_shape(p) for p in params]\n",
    "        ms = [K.zeros(shape) for shape in shapes]\n",
    "        vs = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "\n",
    "            # if a weight tensor (len > 1) use weight normalized parameterization\n",
    "            # this is the only part changed w.r.t. keras.optimizers.Adam\n",
    "            ps = K.get_variable_shape(p)\n",
    "            if len(ps)>1:\n",
    "\n",
    "                # get weight normalization parameters\n",
    "                V, V_norm, V_scaler, g_param, grad_g, grad_V = get_weightnorm_params_and_grads(p, g)\n",
    "\n",
    "                # Adam containers for the 'g' parameter\n",
    "                V_scaler_shape = K.get_variable_shape(V_scaler)\n",
    "                m_g = K.zeros(V_scaler_shape)\n",
    "                v_g = K.zeros(V_scaler_shape)\n",
    "\n",
    "                # update g parameters\n",
    "                m_g_t = (self.beta_1 * m_g) + (1. - self.beta_1) * grad_g\n",
    "                v_g_t = (self.beta_2 * v_g) + (1. - self.beta_2) * K.square(grad_g)\n",
    "                new_g_param = g_param - lr_t * m_g_t / (K.sqrt(v_g_t) + self.epsilon)\n",
    "                self.updates.append(K.update(m_g, m_g_t))\n",
    "                self.updates.append(K.update(v_g, v_g_t))\n",
    "\n",
    "                # update V parameters\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * grad_V\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(grad_V)\n",
    "                new_V_param = V - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "                self.updates.append(K.update(m, m_t))\n",
    "                self.updates.append(K.update(v, v_t))\n",
    "\n",
    "                # if there are constraints we apply them to V, not W\n",
    "                if p in constraints:\n",
    "                    c = constraints[p]\n",
    "                    new_V_param = c(new_V_param)\n",
    "\n",
    "                # wn param updates --> W updates\n",
    "                add_weightnorm_param_updates(self.updates, new_V_param, new_g_param, p, V_scaler)\n",
    "\n",
    "            else: # do optimization normally\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "                self.updates.append(K.update(m, m_t))\n",
    "                self.updates.append(K.update(v, v_t))\n",
    "\n",
    "                new_p = p_t\n",
    "                # apply constraints\n",
    "                if p in constraints:\n",
    "                    c = constraints[p]\n",
    "                    new_p = c(new_p)\n",
    "                self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "\n",
    "def get_weightnorm_params_and_grads(p, g):\n",
    "    ps = K.get_variable_shape(p)\n",
    "\n",
    "    # construct weight scaler: V_scaler = g/||V||\n",
    "    V_scaler_shape = (ps[-1],)  # assumes we're using tensorflow!\n",
    "    V_scaler = K.ones(V_scaler_shape)  # init to ones, so effective parameters don't change\n",
    "\n",
    "    # get V parameters = ||V||/g * W\n",
    "    norm_axes = [i for i in range(len(ps) - 1)]\n",
    "    V = p / tf.reshape(V_scaler, [1] * len(norm_axes) + [-1])\n",
    "\n",
    "    # split V_scaler into ||V|| and g parameters\n",
    "    V_norm = tf.sqrt(tf.reduce_sum(tf.square(V), norm_axes))\n",
    "    g_param = V_scaler * V_norm\n",
    "\n",
    "    # get grad in V,g parameters\n",
    "    grad_g = tf.reduce_sum(g * V, norm_axes) / V_norm\n",
    "    grad_V = tf.reshape(V_scaler, [1] * len(norm_axes) + [-1]) * \\\n",
    "             (g - tf.reshape(grad_g / V_norm, [1] * len(norm_axes) + [-1]) * V)\n",
    "\n",
    "    return V, V_norm, V_scaler, g_param, grad_g, grad_V\n",
    "\n",
    "\n",
    "def add_weightnorm_param_updates(updates, new_V_param, new_g_param, W, V_scaler):\n",
    "    ps = K.get_variable_shape(new_V_param)\n",
    "    norm_axes = [i for i in range(len(ps) - 1)]\n",
    "\n",
    "    # update W and V_scaler\n",
    "    new_V_norm = tf.sqrt(tf.reduce_sum(tf.square(new_V_param), norm_axes))\n",
    "    new_V_scaler = new_g_param / new_V_norm\n",
    "    new_W = tf.reshape(new_V_scaler, [1] * len(norm_axes) + [-1]) * new_V_param\n",
    "    updates.append(K.update(W, new_W))\n",
    "    updates.append(K.update(V_scaler, new_V_scaler))\n",
    "\n",
    "\n",
    "# data based initialization for a given Keras model\n",
    "def data_based_init(model, input):\n",
    "    # input can be dict, numpy array, or list of numpy arrays\n",
    "    if type(input) is dict:\n",
    "        feed_dict = input\n",
    "    elif type(input) is list:\n",
    "        feed_dict = {tf_inp: np_inp for tf_inp,np_inp in zip(model.inputs,input)}\n",
    "    else:\n",
    "        feed_dict = {model.inputs[0]: input}\n",
    "\n",
    "    # add learning phase if required\n",
    "    if model.uses_learning_phase and K.learning_phase() not in feed_dict:\n",
    "        feed_dict.update({K.learning_phase(): 1})\n",
    "\n",
    "    # get all layer name, output, weight, bias tuples\n",
    "    layer_output_weight_bias = []\n",
    "    for l in model.layers:\n",
    "        if hasattr(l, 'W') and hasattr(l, 'b'):\n",
    "            assert(l.built)\n",
    "            layer_output_weight_bias.append( (l.name,l.get_output_at(0),l.W,l.b) ) # if more than one node, only use the first\n",
    "\n",
    "    # iterate over our list and do data dependent init\n",
    "    sess = K.get_session()\n",
    "    for l,o,W,b in layer_output_weight_bias:\n",
    "        print('Performing data dependent initialization for layer ' + l)\n",
    "        m,v = tf.nn.moments(o, [i for i in range(len(o.get_shape())-1)])\n",
    "        s = tf.sqrt(v + 1e-10)\n",
    "        updates = tf.group(W.assign(W/tf.reshape(s,[1]*(len(W.get_shape())-1)+[-1])), b.assign((b-m)/s))\n",
    "        sess.run(updates, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(x):\n",
    "    \"\"\"Scaled Exponential Linear Unit. (Klambauer et al., 2017)\n",
    "    # Arguments\n",
    "        x: A tensor or variable to compute the activation function for.\n",
    "    # References\n",
    "        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n",
    "    \"\"\"\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * K.elu(x, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "\n",
    "inputs = Input((1, 46, 46, seq_len))\n",
    "\n",
    "n_filters = 32\n",
    "l2 = 1e-8\n",
    "\n",
    "x = Permute((1, 4, 2, 3))(inputs)\n",
    "\n",
    "x = Conv3D(n_filters, 5, strides=(2, 2, 2), data_format='channels_first', activation=selu,\n",
    "           kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2))(x)\n",
    "x = MaxPooling3D((1, 2, 2), data_format='channels_first')(x)\n",
    "\n",
    "x = Conv3D(n_filters * 2, 3, strides=(1, 1, 1), data_format='channels_first', activation=selu,\n",
    "           kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2))(x)\n",
    "x = MaxPooling3D((2, 2, 2), data_format='channels_first')(x)\n",
    "\n",
    "x = Conv3D(n_filters * 4, 3, strides=(1, 1, 1), data_format='channels_first', activation=selu,\n",
    "           kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2))(x)\n",
    "\n",
    "x = AveragePooling3D((28, 2, 2), data_format='channels_first')(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dropout(.2)(x)\n",
    "x = Dense(n_filters * 4, activation=selu,\n",
    "          kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2))(x)\n",
    "\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=AdamWithWeightnorm(0.0005), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    return 0.0005 if epoch < 100 else 0.0005 * .25\n",
    "\n",
    "scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "batch_size = 32\n",
    "train_gen = data_gen(h5py.File(filter_data_path, 'r'), train_indices, batch_size, augment=True)\n",
    "val_gen = data_gen(h5py.File(filter_data_path, 'r'), val_indices, batch_size)\n",
    "\n",
    "hist = model.fit_generator(train_gen, len(train_indices) // batch_size, 200, \n",
    "                           validation_data=val_gen, validation_steps=len(val_indices) // batch_size,\n",
    "                           callbacks=[scheduler])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
